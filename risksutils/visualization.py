# -*- coding: utf-8 -*-

from collections import namedtuple
from functools import wraps
from scipy.stats import beta
from scipy.special import logit
import numpy as np
import pandas as pd
import holoviews as hv
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.isotonic import IsotonicRegression


def _set_options(func):
    """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        diagramm = func(*args, **kwargs)
        for bnd, opts in [('matplotlib', matplotlib_opts),
                          ('bokeh', bokeh_opts)]:
            if (bnd in hv.Store._options  # pylint: disable=protected-access
                    and bnd == hv.Store.current_backend):
                return diagramm.opts(opts)
        return diagramm
    return wrapper


colors = hv.Cycle([  # pylint: disable=invalid-name
    '#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',
    '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])


matplotlib_opts = {  # pylint: disable=invalid-name
    'Scatter.Weight_of_evidence': {                  # woe_line
        'plot': dict(show_grid=True),
    },
    'NdOverlay.Objects_rate': {                      # distribution
        'plot': dict(xrotation=45, legend_cols=1, legend_position='right'),
    },
    'Spread.Objects_rate': {                         # distribution
        'plot': dict(show_legend=True, show_grid=True),
        'style': dict(facecolor=colors),
    },
    'Overlay.Woe_Stab': {                            # woe_stab
        'plot': dict(legend_position='right'),
    },
    'Curve.Weight_of_evidence': {                    # woe_stab
        'style': dict(color=colors),
    },
    'Spread.Confident_Intervals': {                  # woe_stab
        'plot': dict(show_grid=True, xrotation=45),
        'style': dict(facecolor=colors, alpha=0.3),
    },
    'Curve.Isotonic': {                              # isotonic
        'plot': dict(show_grid=True),
    },
    'Area.Confident_Intervals': {                    # isotonic
        'style': dict(alpha=0.5),
    },
}

bokeh_opts = {  # pylint: disable=invalid-name
    'Scatter.Weight_of_evidence': {                  # woe_line
        'plot': dict(show_grid=True, tools=['hover']),
    },
    'NdOverlay.Objects_rate': {                      # distribution
        'plot': dict(xrotation=45, legend_position='right', width=450),
    },
    'Spread.Objects_rate': {                         # distribution
        'plot': dict(show_legend=True, show_grid=True, tools=['hover']),
        'style': dict(color=colors),
    },
    'Overlay.Woe_Stab': {                            # woe_stab
        'plot': dict(legend_position='right', width=450),
    },
    'Curve.Weight_of_evidence': {                    # woe_stab
        'plot': dict(tools=['hover']),
        'style': dict(color=colors),
    },
    'Spread.Confident_Intervals': {                  # woe_stab
        'plot': dict(show_grid=True, xrotation=45),
        'style': dict(color=colors, alpha=0.3),
    },
    'Curve.Isotonic': {                              # isotonic
        'plot': dict(show_grid=True, tools=['hover']),
    },
    'Area.Confident_Intervals': {                    # isotonic
        'style': dict(alpha=0.5),
    },
}


@_set_options
def woe_line(df, feature, target, num_buck=10):
    """–ì—Ä–∞—Ñ–∏–∫ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ WoE –æ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∞

    **–ê—Ä–≥—É–º–µ–Ω—Ç—ã**

    df : pandas.DataFrame
        —Ç–∞–±–ª–∏—Ü–∞ —Å –¥–∞–Ω–Ω—ã–º–∏
    feature : str
        –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞
    target : str
        –Ω–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    num_buck : int
        –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞–∫–µ—Ç–æ–≤

    **–†–µ–∑—É–ª—å—Ç–∞—Ç**

    scatter * errors * line : holoviews.Overlay
    """
    df_agg = _aggregate_data_for_woe_line(df, feature, target, num_buck)

    scatter = hv.Scatter(data=df_agg, kdims=[feature],
                         vdims=['woe'], group='Weight of evidence')
    errors = hv.ErrorBars(data=df_agg, kdims=[feature],
                          vdims=['woe', 'woe_u', 'woe_b'],
                          group='Confident Intervals')
    line = hv.Curve(data=df_agg, kdims=[feature], vdims=['logreg'],
                    group='Logistic interpolations')
    diagram = hv.Overlay(items=[scatter, errors, line],
                         group='Woe line',
                         label=feature)

    return diagram


@_set_options
def woe_stab(df, feature, target, date, num_buck=10, date_freq='MS'):
    """–ì—Ä–∞—Ñ–∏–∫ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ WoE –ø—Ä–∏–∑–Ω–∞–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏

    **–ê—Ä–≥—É–º–µ–Ω—Ç—ã**

    df : pandas.DataFrame
        —Ç–∞–±–ª–∏—Ü–∞ —Å –¥–∞–Ω–Ω—ã–º–∏

    feature : str
        –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞

    target : str
        –Ω–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π

    date : str
        –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ–ª—è —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º

    num_buck : int
        –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞–∫–µ—Ç–æ–≤

    date_ferq : str
        –¢–∏–ø –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'MS' - –Ω–∞—á–∞–ª–æ –º–µ—Å—è—Ü–∞)

    **–†–µ–∑—É–ª—å—Ç–∞—Ç**

    curves * spreads : holoviews.Overlay
    """

    df_agg = _aggregate_data_for_woe_stab(df, feature, target, date,
                                          num_buck, date_freq)

    data = hv.Dataset(df_agg, kdims=['bucket', date],
                      vdims=['woe', 'woe_b', 'woe_u'])
    confident_intervals = (data.to.spread(kdims=[date],  # pylint: disable=no-member
                                          vdims=['woe', 'woe_b', 'woe_u'],
                                          group='Confident Intervals')
                           .overlay('bucket'))
    woe_curves = (data.to.curve(kdims=[date], vdims=['woe'],  # pylint: disable=no-member
                                group='Weight of evidence')
                  .overlay('bucket'))
    diagram = hv.Overlay(items=[confident_intervals * woe_curves],
                         group='Woe Stab',
                         label=feature)
    return diagram


@_set_options
def distribution(df, feature, date, num_buck=10, date_freq='MS'):
    """–ì—Ä–∞—Ñ–∏–∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏

    **–ê—Ä–≥—É–º–µ–Ω—Ç—ã**

    df : pandas.DataFrame
        —Ç–∞–±–ª–∏—Ü–∞ —Å –¥–∞–Ω–Ω—ã–º–∏

    feature : str
        –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞

    date : str
        –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ–ª—è —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º

    num_buck : int
        –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞–∫–µ—Ç–æ–≤

    date_ferq : str
        –¢–∏–ø –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'MS' - –Ω–∞—á–∞–ª–æ –º–µ—Å—è—Ü–∞)

    **–†–µ–∑—É–ª—å—Ç–∞—Ç**

    spreads : holoviews.NdOverlay
    """

    df_agg = _aggregate_data_for_distribution(df, feature, date,
                                              num_buck, date_freq)

    obj_rates = (hv.Dataset(df_agg, kdims=['bucket', date],  # pylint: disable=no-member
                            vdims=['objects_rate', 'obj_rate_l', 'obj_rate_u'])
                 .to.spread(kdims=[date],
                            vdims=['objects_rate', 'obj_rate_l', 'obj_rate_u'],
                            group='Objects rate',
                            label=feature)
                 .overlay('bucket'))

    return obj_rates


@_set_options
def isotonic(df, predict, target, calibrations_data=None):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏

    **–ê—Ä–≥—É–º–µ–Ω—Ç—ã**

    df : pandas.DataFrame
        —Ç–∞–±–ª–∏—Ü–∞ —Å –¥–∞–Ω–Ω—ã–º–∏

    predict : str
        –ø—Ä–æ–≥–Ω–æ–∑–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å

    target : str
        –±–∏–Ω–∞—Ä–Ω–∞—è (0, 1) —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è

    calibrations_data : pandas.DataFrame
        —Ç–∞–±–ª–∏—Ü–∞ —Å –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞–º–∏

    **–†–µ–∑—É–ª—å—Ç–∞—Ç**

    area * curve * [curve] : holoviews.Overlay
    """

    df_agg = _aggregate_data_for_isitonic(df, predict, target)

    confident_intervals = hv.Area(df_agg, kdims=['predict'],
                                  vdims=['ci_l', 'ci_h'],
                                  group='Confident Intervals')
    curve = hv.Curve(df_agg, kdims=['predict'], vdims=['isotonic'],
                     group='Isotonic')

    if calibrations_data is not None and target in calibrations_data.columns:
        calibration = hv.Curve(
            data=calibrations_data[['predict', target]].values,
            kdims=['predict'],
            vdims=['target'],
            group='Calibration',
            label='calibration'
        )
        return hv.Overlay(items=[curve, confident_intervals, calibration],
                          group='Isotonic', label=predict)
    return hv.Overlay(items=[curve, confident_intervals],
                      group='Isotonic', label=predict)


def cross_tab(df, feature1, feature2, target,
              num_buck1=10, num_buck2=10, min_sample=100,
              compute_iv=False):
    """–ö—Ä–æ—Å—Å—Ç–∞–±—É–ª—è—Ü–∏—è –ø–∞—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –±–∏–Ω–∞—Ä–Ω–æ–π —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π

    **–ê—Ä–≥—É–º–µ–Ω—Ç—ã**

    df : pandas.DataFrame
        —Ç–∞–±–ª–∏—Ü–∞ —Å –¥–∞–Ω–Ω—ã–º–∏

    feature1 : str
        –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞ 1

    feature2 : str
        –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞ 2

    target : str
        –Ω–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π

    num_buck1 : int
        –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞–∫–µ—Ç–æ–≤ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–∞ 1

    num_buck2 : int
        –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞–∫–µ—Ç–æ–≤ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–∞ 2

    min_sample : int
        –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –¥–ª—è
        –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–æ–ª–∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –≤ —è—á–µ–π–∫–µ

    compute_iv : bool
        –Ω—É–∂–Ω–æ –ª–∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—Ç—å information value –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

    **–†–µ–∑—É–ª—å—Ç–∞—Ç**

    (rates, counts) : (pandas.Styler, pandas.Styler)
    """

    f1_buck, f2_buck, target = (
        df
        .loc[df[target].dropna().index]
        .reset_index()
        .pipe(lambda x: (_make_bucket(x[feature1], num_buck1),
                         _make_bucket(x[feature2], num_buck2),
                         x[target]))
    )

    f1_buck_names = dict(zip(range(len(f1_buck)), f1_buck.categories))
    f2_buck_names = dict(zip(range(len(f2_buck)), f2_buck.categories))

    rates = (pd.crosstab(f1_buck.codes, f2_buck.codes, target, aggfunc=np.mean,
                         margins=True, rownames=[feature1], colnames=[feature2])
             .rename(index=f1_buck_names, columns=f2_buck_names))

    counts = (pd.crosstab(f1_buck.codes, f2_buck.codes,
                          margins=True, rownames=[feature1], colnames=[feature2])
              .rename(index=f1_buck_names, columns=f2_buck_names))

    if compute_iv:
        information_val = _iv_for_cross_tab(rates, counts)

    rates[counts < min_sample] = np.nan

    rates, counts = _add_style_for_cross_tab(rates, counts)

    if compute_iv:
        return _TupleHTML((information_val, rates, counts))
    return _TupleHTML((rates, counts))


def _aggregate_data_for_woe_line(df, feature, target, num_buck):
    df = df[[feature, target]].dropna()

    df_agg = (
        df.assign(bucket=lambda x: _make_bucket(x[feature], num_buck),
                  obj_count=1)
        .groupby('bucket', as_index=False)
        .agg({target: 'sum', 'obj_count': 'sum', feature: 'mean'})
        .dropna()
        .rename(columns={target: 'target_count'})
        .assign(obj_total=lambda x: x['obj_count'].sum(),
                target_total=lambda x: x['target_count'].sum())
        .assign(obj_rate=lambda x: x['obj_count'] / x['obj_total'],
                target_rate=lambda x: x['target_count'] / x['obj_count'],
                target_rate_total=lambda x: x['target_total'] / x['obj_total'])
        .assign(woe=lambda x: _woe(x['target_rate'], x['target_rate_total']),
                woe_lo=lambda x: _woe_ci(x['target_count'], x['obj_count'],
                                         x['target_rate_total'])[0],
                woe_hi=lambda x: _woe_ci(x['target_count'], x['obj_count'],
                                         x['target_rate_total'])[1])
        .assign(woe_u=lambda x: x['woe_hi'] - x['woe'],
                woe_b=lambda x: x['woe'] - x['woe_lo'])
        .loc[:, [feature, 'obj_count', 'target_rate', 'woe', 'woe_u', 'woe_b']]
    )

    # Logistic interpolation
    clf = Pipeline([
        ('scalle', StandardScaler()),
        ('log_reg', LogisticRegression(C=1))
    ])
    clf.fit(df[[feature]], df[target])
    df_agg['logreg'] = _woe(clf.predict_proba(df_agg[[feature]])[:, 1],
                            np.repeat(df[target].mean(), df_agg.shape[0]))

    return df_agg


def _aggregate_data_for_woe_stab(df, feature, target,
                                 date, num_buck, date_freq):
    return (
        df.loc[lambda x: x[[date, target]].notnull().all(axis=1)]
        .loc[:, [feature, target, date]]
        .assign(bucket=lambda x: _make_bucket(x[feature], num_buck),
                obj_count=1)
        .groupby(['bucket', pd.Grouper(key=date, freq=date_freq)])
        .agg({target: 'sum', 'obj_count': 'sum'})
        .reset_index()
        .assign(
            obj_total=lambda x: (
                x.groupby(pd.Grouper(key=date, freq=date_freq))
                ['obj_count'].transform('sum')),
            target_total=lambda x: (
                x.groupby(pd.Grouper(key=date, freq=date_freq))
                [target].transform('sum')))
        .assign(obj_rate=lambda x: x['obj_count'] / x['obj_total'],
                target_rate=lambda x: x[target] / x['obj_count'],
                target_rate_total=lambda x: x['target_total'] / x['obj_total'])
        .assign(woe=lambda x: _woe(x['target_rate'], x['target_rate_total']),
                woe_lo=lambda x: _woe_ci(x[target], x['obj_count'],
                                         x['target_rate_total'])[0],
                woe_hi=lambda x: _woe_ci(x[target], x['obj_count'],
                                         x['target_rate_total'])[1])
        .assign(woe_u=lambda x: x['woe_hi'] - x['woe'],
                woe_b=lambda x: x['woe'] - x['woe_lo'])
    )


def _aggregate_data_for_distribution(df, feature, date,
                                     num_buck, date_freq):
    return (
        df.loc[:, [feature, date]]
        .assign(bucket=lambda x: _make_bucket(x[feature], num_buck),
                obj_count=1)
        .groupby(['bucket', pd.Grouper(key=date, freq=date_freq)])
        .agg({'obj_count': 'sum'})
        .pipe(lambda x:  # –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –≤—Å–µ –Ω–µ –ø–æ—è–≤–∏–≤—à–µ–µ—Å—è –∑–Ω–∞—á–µ–Ω–∏—è
              x.reindex(pd.MultiIndex.from_product(x.index.levels,
                                                   names=x.index.names),
                        fill_value=0))
        .reset_index()
        .assign(
            obj_total=lambda x: (
                x.groupby(pd.Grouper(key=date, freq=date_freq))
                ['obj_count'].transform('sum')))
        .assign(obj_rate=lambda x: x['obj_count'] / x['obj_total'])
        .sort_values([date, 'bucket'])
        .reset_index(drop=True)
        .assign(objects_rate=lambda x:
                x.groupby(date).apply(
                    lambda y: y.obj_rate.cumsum().to_frame())
                .reset_index(drop=True))
        .assign(obj_rate_u=0,
                obj_rate_l=lambda x: x['obj_rate'])
    )


def _make_bucket(series, num_buck):
    bucket = np.ceil(series.rank(pct=True) * num_buck).fillna(num_buck + 1)
    bucket = pd.Categorical(bucket, categories=np.sort(bucket.unique()),
                            ordered=True)
    agg = series.groupby(bucket).agg(['min', 'max'])

    def _format_buck(row):
        if row.name == num_buck + 1:
            return 'missing'
        if row['min'] == row['max']:
            return _format_val(row['min'])
        return '[{}; {}]'.format(
            _format_val(row['min']),
            _format_val(row['max'])
        )

    names = agg.apply(_format_buck, axis=1)
    return bucket.rename_categories(names.to_dict())


def _format_val(x, precision=3):
    """format a value for _make_buck

    >>> _format_val(0.00001)
    '1e-05'
    >>> _format_val(2.00001)
    '2.0'
    >>> _format_val(1000.0)
    '1000'
    >>> _format_val('foo')
    'foo'
    """
    if isinstance(x, float):
        if np.equal(np.mod(x, 1), 0):
            return '%d' % x
        if not np.isfinite(x):
            return '%s' % x
        frac, whole = np.modf(x)
        if whole == 0:
            digits = -int(np.floor(np.log10(abs(frac)))) - 1 + precision
        else:
            digits = precision
        return '%s' % np.around(x, digits)
    return '%s' % x


def _woe(tr, tr_all):
    '''Compute Weight Of Evidence from target rates

    >>> _woe(np.array([0.1, 0, 0.1]), np.array([0.5, 0.5, 0.1]))
    array([-2.19722458, -6.90675478,  0.        ])
    '''
    tr, tr_all = np.clip([tr, tr_all], 0.001, 0.999)
    return logit(tr) - logit(tr_all)


def _woe_ci(t, cnt, tr_all, alpha=0.05):
    '''Compute confident bound for WoE'''
    tr_lo, tr_hi = _clopper_pearson(t, cnt, alpha)
    woe_lo = _woe(tr_lo, tr_all)
    woe_hi = _woe(tr_hi, tr_all)
    return woe_lo, woe_hi


def _clopper_pearson(k, n, alpha=0.32):
    """Clopper Pearson intervals are a conservative estimate

    See also
    http://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval

    >>> _clopper_pearson(0, 10)
    (0.0, 0.16744679259812678)
    """
    lo = beta.ppf(alpha / 2, k, n - k + 1)
    hi = beta.ppf(1 - alpha / 2, k + 1, n - k)
    lo = np.nan_to_num(lo)
    hi = 1 - np.nan_to_num(1 - hi)
    return lo, hi


def _aggregate_data_for_isitonic(df, predict, target):
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–∏—Å–æ–≤–∞–Ω–∏—è Isotonic –¥–∏–∞–≥—Ä–∞–º–º—ã"""
    reg = IsotonicRegression()
    return (df[[predict, target]]                # –≤—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –¥–≤–∞ –ø–æ–ª—è
            .dropna()                            # –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–ø—É—Å—Ç—ã–µ
            .rename(columns={predict: 'predict',
                             target: 'target'})  # –º–µ–Ω—è–µ–º –∏—Ö –Ω–∞–∑–≤–∞–Ω–∏—è
            .assign(isotonic=lambda df:          # –∑–Ω–∞—á–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–∞ IR
                    reg.fit_transform(           # –æ–±—É—á–∞–µ–º –∏ —Å—á–∏—Ç–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑.
                        X=(df['predict'] +          # üî´IR –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å
                           1e-7 * np.random.rand(len(df))),
                        y=df['target']           # –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–º–∏—Å—è –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                    ))                           # –ø–æ—ç—Ç–æ–º—É –∫–æ—Å—Ç—ã–ª—å–Ω–æ –¥–µ–ª–∞–µ–º –∏—Ö
            .groupby('isotonic')                 # —Ä–∞–∑–Ω—ã–º–∏.
            .agg({'target': ['sum', 'count'],    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è ir
                  'predict': ['min', 'max']})    # –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º target
            .reset_index()
            .pipe(_compute_confident_intervals)   # –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
            .pipe(_stack_min_max))                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç


def _compute_confident_intervals(df):
    """–î–æ–±–∞–≤–ª—è–µ–º –≤ —Ç–∞–±–ª–∏—Ü—É –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã"""
    df['ci_l'], df['ci_h'] = _clopper_pearson(
        k=df['target']['sum'],
        n=df['target']['count'],
        alpha=0.05,
    )
    return df


def _stack_min_max(df):
    """–ü–µ—Ä–µ–≥—Ä—É–ø–ø–∏—Ä–æ–≤—ã–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Ç–∞–±–ª–∏—Ü–µ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ —Ä–∏—Å–æ–≤–∞–Ω–∏—è"""
    stack = (df['predict']                 # predict - –ú—É–ª—å—Ç–∏ –ò–Ω–¥–µ–∫—Å,
             .stack()                      # –ö–∞–∂–¥–æ–π —Å—Ç—Ä–æ—á–∫–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º
                                           # –¥–≤–µ —Å—Ç—Ä–æ—á–∫–∏ —Å–æ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
             .reset_index(1, drop=True)    # –¥–ª—è min –∏ –¥–ª—è max,
             .rename('predict'))           # –∞ –ø–æ—Ç–æ–º –º–µ–Ω—è–º –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ–ª—è
    df = pd.concat([stack, df['isotonic'],
                    df['ci_l'],
                    df['ci_h']], axis=1)
    df['ci_l'] = df['ci_l'].cummax()       # –î–µ–ª–∞–µ–º –≥—Ä–∞–Ω–∏—Ü—ã –º–æ–Ω–æ—Ç–æ–Ω–Ω—ã–º–∏
    df['ci_h'] = df[::-1]['ci_h'].cummin()[::-1]
    return df


def _hex_to_rgb(color):
    """
    >>> _hex_to_rgb('#dead13')
    (222, 173, 19)
    """
    return tuple(int(color.lstrip('#')[i:i + 2], 16) for i in (0, 2, 4))


def _rgb_to_hex(rgb):
    """
    >>> _rgb_to_hex((222, 173, 19))
    '#dead13'
    """
    return '#%02x%02x%02x' % tuple(rgb)


def _color_interpolate(values, bounds_colors):
    """–ò–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä—É–µ–º —Ü–≤–µ—Ç –∏—Å—Ö–æ–¥—è –∏–∑ –≥—Ä–∞–Ω–∏—Ü

    >>> _color_interpolate([1, 1.5], [(0, '#010101'), (2, '#050905')])
    bound
    0.0    #010101
    1.0    #030503
    1.5    #040704
    2.0    #050905
    dtype: object
    """
    return (
        pd.DataFrame
        .from_records(bounds_colors, columns=['bound', 'color'])
        .groupby('bound')
        .first()
        .loc[:, 'color']
        .apply(_hex_to_rgb)
        .apply(pd.Series)
        .append(pd.DataFrame(index=pd.Series(values, name='bound')))
        .sort_index()
        .interpolate('index')
        .fillna(method='ffill')
        .fillna(method='bfill')
        .astype(np.int)
        .reset_index()
        .drop_duplicates()
        .set_index('bound')
        .apply(_rgb_to_hex, axis=1)
    )


class _TupleHTML(tuple):
    def _repr_html_(self):
        return '<br>'.join(i._repr_html_()  # pylint: disable=protected-access
                           if hasattr(i, '_repr_html_')
                           else repr(i)
                           for i in self)


def _add_style_for_cross_tab(rates, counts):

    rates_colors = _color_interpolate(
        values=rates.unstack().dropna(),
        bounds_colors=[
            (rates.unstack().min(), '#63bf7a'),     # green
            (rates.unstack().median(), '#ffea84'),  # yellow
            (rates.unstack().max(), '#f7686b')      # red
        ]
    )

    counts_colors = _color_interpolate(
        values=counts.unstack().dropna(),
        bounds_colors=[
            (counts.iloc[:-1, :-1].unstack().min(), '#f2f2f2'),    # light grey
            (counts.iloc[:-1, :-1].unstack().median(), '#bfbfbf'),
            (counts.iloc[:-1, :-1].unstack().max(), '#7f7f7f')     # grey
        ]
    )

    rotate_col_heading_style = dict(
        selector="th[class*='col_heading']",
        props=[("-webkit-transform", "rotate(-45deg)"),
               ('max-width', '50px')]
    )

    rates = (
        rates.style
        .applymap(lambda x: 'background-color: %s' % rates_colors[x]
                  if x == x else '')  # pylint: disable=comparison-with-itself
        .format("{:.2%}")
        .set_table_styles([rotate_col_heading_style])
    )

    counts = (
        counts.style
        .applymap(lambda x: 'background-color: %s' % counts_colors[x]
                  if x == x else '')  # pylint: disable=comparison-with-itself
        .set_table_styles([rotate_col_heading_style])
    )

    return rates, counts


def _iv_from(rates, counts, all_rate, all_count):
    return (
        pd.DataFrame({'tr': rates, 'cnt': counts})
        .dropna()
        .assign(tr=lambda x: np.clip(x['tr'], 0.001, 0.999))
        .eval('     (    (tr/{all_tr}) -    ((1-tr)/(1-{all_tr})) )'
              '   * ( log(tr/{all_tr}) - log((1-tr)/(1-{all_tr})) )'
              '   * ( cnt/{all_cnt})'
              ''.format(all_tr=all_rate, all_cnt=all_count))
        .sum()
    )


def _iv_for_cross_tab(rates, counts):
    return (
        pd.DataFrame
        .from_records([
            (rates.index.name, _iv_from(
                rates.iloc[:-1, -1], counts.iloc[:-1, -1],
                rates.iloc[-1, -1], counts.iloc[-1, -1])),
            (rates.columns.name, _iv_from(
                rates.iloc[-1, :-1], counts.iloc[-1, :-1],
                rates.iloc[-1, -1], counts.iloc[-1, -1])),
            ('%s %s' % (rates.index.name, rates.columns.name), _iv_from(
                rates.iloc[:-1, :-1].unstack(),
                counts.iloc[:-1, :-1].unstack(),
                rates.iloc[-1, -1], counts.iloc[-1, -1]))
        ])
        .rename(columns={0: 'feature', 1: 'IV'})
        .set_index('feature')
    )


_Plot = namedtuple('Plot', ['selector', 'diagram'])


class InteractiveIsotonic():
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏

    **–ê—Ä–≥—É–º–µ–Ω—Ç—ã**

    data : pandas.DataFrame
        —Ç–∞–±–ª–∏—Ü–∞ —Å –¥–∞–Ω–Ω—ã–º–∏

    pdims : list
        —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–æ–ª–±—Ü–æ–≤ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏

    tdims : list
        —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–æ–ª–±—Ü–æ–≤ —Å —Ü–µ–ª–µ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏

    ddims : list
        —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–æ–ª–±—Ü–æ–≤ —Å –¥–∞—Ç–∞–º–∏

    gdims : list
        —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–æ–ª–±—Ü–æ–≤ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏ –ø–æ–ª—è–º–∏

    calibrations_data : pandas.DataFrame
        —Ç–∞–±–ª–∏—Ü–∞ —Å–æ–¥–µ—Ä–∂–∞—â–∞—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞
        –≤ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö tdims
        –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±–µ—Ü c –∏–º–µ–Ω–µ–º predict
        ::
            tdims = ['t1', 't2']
            calibrations_data = pd.DataFrame({
                'predict': [0, 0.3, 0.6],
                't1': [0, 0.1, 0.2],
                't2': [0, 0.4, 0.8]
            })
        –µ—Å–ª–∏ –∞—Ä–≥—É–º–µ–Ω—Ç –∑–∞–¥–∞–Ω, —Ç–æ –Ω–∞ –¥–∏–∞–≥—Ä–∞–º–º–∞—Ö isotonic
        –±—É–¥—É—Ç –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ –∫–∞–ª–∏–±—Ä–æ–≤–æ–∫

    **–†–µ–∑—É–ª—å—Ç–∞—Ç**

    diagram
        –æ–±—ä–µ–∫—Ç —Å –Ω–∞–±–æ—Ä–æ–º —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –¥–∏–∞–≥—Ä–∞–º–º

        isotonic : hv.DynamicMap
            –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å —á–∞—Å—Ç–æ—Ç—ã –Ω–∞—Å—Ç—É–ø–ª–µ–Ω–∏—è —Å–æ–±—ã—Ç–∏—è
            –æ—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞. –°–æ–¥–µ—Ä–∂–∏—Ç –≤–∏–¥–∂–µ—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ
            –≤—ã–±–æ—Ä–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞ (pdims) –∏ –¥–ª—è –≤—ã–±–æ—Ä–∞ —Ü–µ–ª–µ–≤–æ–π
            –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (tdims)
        –æ–±—Ä–∞—â–∞—Ç—å—Å—è –∫ –¥–∏–∞–≥—Ä–∞–º–º–∞–º –Ω—É–∂–Ω–æ, –∫–∞–∫ –∫ –∞—Ç—Ä–∏–±—É—Ç–∞–º
        ::
            diagram.isotonic

        –¥–æ—Å—Ç—É–ø–Ω—ã –¥–∏–∞–≥—Ä–∞–º–º—ã –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π, —É–∫–∞–∑–∞–Ω–Ω—ã—Ö
        –≤ gdims, –∏ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö, —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –≤ ddims
        –æ–±—Ä–∞—â–∞—Ç—å—Å—è –∫ –Ω–∏–º –º–æ–∂–Ω–æ –ø–æ –∏–º–µ–Ω–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä
        ::
            ddims = ['request_dt', 'response_dt']
            diagram.request_dt
            diagram.response_dt
        –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –¥–∏–∞–≥—Ä–∞–º–º–∞—Ö –º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –ø–æ–¥–≤—ã–±–æ—Ä–∫—É, —Å –ø–æ–º–æ—â—å—é
        –≤–∏–¥–∂–µ—Ç–æ–≤ –¥–∏–∞–≥—Ä–∞–º–º bokeh, —Ç–æ–≥–¥–∞ –ø–µ—Ä–µ—Å—á–∏—Ç–∞—é—Ç—Å—è –∏ –≤—Å–µ
        –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –¥–∏–∞–≥—Ä–∞–º–º—ã

    """

    def __init__(self, data, pdims, tdims, ddims=None, gdims=None,
                 calibrations_data=None):
        self.data = data
        self._pdims = pdims
        self._tdims = tdims
        self._gdims = gdims if gdims else []
        self._ddims = ddims if ddims else []
        self._calibrations_data = calibrations_data
        self._check_fields()        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç—ã –ø–æ–ª–µ–π
        self._diagrams = {}         # –ó–¥–µ—Å—å –±—É–¥–µ–º —Ö—Ä–∞–Ω–∏—Ç—å –¥–∏–∞–≥—Ä–∞–º–º—ã
        self._make_bars_static()    # –°–æ–∑–¥–∞–µ–º –¥–∏–∞–≥—Ä–∞–º–º—ã —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
        self._make_area_static()    # –° –¥–∞—Ç–∞–º–∏
        self._make_charts()         # –ö–æ–Ω–≤–µ—Ä—Ç–∏–º –∏—Ö –≤ –≥–æ—Ç–æ–≤—ã–µ –¥–∏–∞–≥—Ä–∞–º–º—ã
        self._make_isotinic_plot()

    def _get_count(self, dim, conditions=None):

        if conditions is None:
            df = self.data
        else:
            df = self.data.loc[conditions]

        return (df.groupby(dim)
                .size()
                .reset_index()
                .rename(columns={0: 'count'}))

    def _make_bars_static(self):
        """–°–æ–∑–¥–∞–µ–º —Å—Ç–æ–ª–±—á–∞—Ç—ã–µ –¥–∏–∞–≥—Ä–∞–º–º—ã —Å –≤—ã–±–æ—Ä–æ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        for dim in self._gdims:
            df = self._get_count(dim)
            diagram = (hv.Bars(df, kdims=[dim], vdims=['count'])
                       .opts(plot=dict(tools=['tap'])))
            selector = (hv.streams
                        .Selection1D(source=diagram)
                        .rename(index=dim))
            self._diagrams[dim] = _Plot(selector, diagram)

    def _make_area_static(self):
        """–°–æ–∑–¥–∞–µ–º –¥–∏–∞–≥—Ä–∞–º–º—ã —Å –≤—ã–±–æ—Ä–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –¥–∞—Ç"""
        for dim in self._ddims:
            df = self._get_count(dim)
            diagram = hv.Area(df, kdims=[dim], vdims=['count'])
            selector = (hv.streams
                        .BoundsX(source=diagram)
                        .rename(boundsx=dim))
            self._diagrams[dim] = _Plot(selector, diagram)

    def _conditions(self, **kwargs):
        """–ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —É—Å–ª–æ–≤–∏—è –¥–ª—è –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏ –∏–∑ —Å—Ç–∞—Ç–∏—á–Ω—ã—Ö –¥–∏–∞–≥—Ä–∞–º–º"""
        conditions = np.repeat(True, len(self.data))  # –°–Ω–∞—á–∞–ª–∞ –∑–∞–¥–∞–µ–º True

        for dim, value in kwargs.items():           # –ù–∞–∑–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
            if dim in self._gdims:                  # —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –ø–æ–ª—è–º–∏
                _, diagram = self._diagrams[dim]
                categories = diagram.data.loc[value][dim]
                if not categories.empty:
                    conditions &= self.data[dim].isin(categories)
            elif dim in self._ddims:
                if value:
                    left, right = value
                    conditions &= self.data[dim].between(left, right)
        return conditions

    def _make_charts(self):
        """–°–æ–∑–¥–∞–µ–º –¥–∏–∞–≥—Ä–∞–º–º—ã –≤–º–µ—Å—Ç–µ —Å –º–µ–Ω—è—é—â–µ–π—Å—è"""
        for dim in self._gdims + self._ddims:
            self.__dict__[dim] = (             # –î–æ–±–∞–≤–ª—è–µ–º –¥–∏–∞–≥—Ä–∞–º–º—É –≤ –∞—Ç—Ä–∏–±—É—Ç—ã
                self._diagrams[dim].diagram *  # self. –ù–µ–±–µ–∑–æ–ø–∞—Å–Ω–æ, –µ—Å–ª–∏
                self._make_one_chart(dim))     # —É–∂–µ —á—Ç–æ-—Ç–æ –µ—Å—Ç—å —Å –∏–º–µ–Ω–µ–º

    def _make_one_chart(self, dim):
        """–û–¥–Ω–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ–º–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞"""

        selectors = [s for s, d in self._diagrams.values()]

        if dim in self._ddims:
            diagram_type = hv.Area
        elif dim in self._gdims:
            diagram_type = hv.Bars

        def bar_chart(**kwargs):
            data = self._get_count(dim, self._conditions(**kwargs))
            return diagram_type(data, kdims=[dim], vdims=['count'])

        return hv.DynamicMap(bar_chart, streams=selectors)

    def _make_isotinic_plot(self):
        """–°–æ–∑–¥–∞–µ–º –¥–∏–∞–≥—Ä–∞–º–º—É —Å IR"""

        kdims = [hv.Dimension('predict', values=self._pdims),
                 hv.Dimension('target', values=self._tdims)]

        selectors = [s for s, d in self._diagrams.values()]

        def chart(target, predict, **kwargs):
            condisions = self._conditions(**kwargs)
            data = self.data.loc[condisions]
            return isotonic(data, predict, target, self._calibrations_data)

        iso_chart = hv.DynamicMap(chart, kdims=kdims, streams=selectors)
        self.__dict__['isotonic'] = iso_chart

    def _check_fields(self):
        """–ü—Ä–æ–≤–µ—Ä—è–ª—å—â–∏–∫ —Ñ–æ—Ä–º–∞—Ç–∞"""
        assert isinstance(self.data, pd.DataFrame), 'data must be DataFrame'
        assert self._pdims is not None, '{} must be not None'.format('pdims')
        assert self._tdims is not None, '{} must be not None'.format('tdims')

        for dims in [self._gdims, self._tdims, self._ddims, self._pdims]:
            assert isinstance(dims, list), '{} must be list'.format(dims)
            for col in dims:
                assert col in self.data.columns, '{} must be a column of data'
